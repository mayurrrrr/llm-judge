"""
LLM Evaluation Playground - Streamlit App
"""
import streamlit as st
import os
from dotenv import load_dotenv
from pydantic import ValidationError

from models import ModelAnswer
from judge import configure_gemini, evaluate_with_gemini
from logger import log_evaluation, get_evaluation_history


# Load environment variables
load_dotenv()

# Page configuration
st.set_page_config(
    page_title="LLM Evaluation Playground",
    page_icon="üßæ",
    layout="wide"
)

# Title
st.title("üßæ LLM Evaluation Playground")
st.markdown("Evaluate model-generated outputs using LLM-as-a-Judge and Pydantic validation")

# Sidebar for configuration
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    # API Key input
    api_key = st.text_input(
        "Google API Key",
        type="password",
        value=os.getenv("GOOGLE_API_KEY", ""),
        help="Enter your Google Gemini API key"
    )
    
    if api_key:
        try:
            configure_gemini(api_key)
            st.success("‚úÖ API Key configured")
        except Exception as e:
            st.error(f"‚ùå Error configuring API: {str(e)}")
    else:
        st.warning("‚ö†Ô∏è Please enter your Google API Key")
    
    st.divider()
    
    # Model selection
    model_options = {
        "gemini-2.5-flash": "‚ö° Flash - Balanced speed and quality (recommended)",
        "gemini-2.5-pro": "üß† Pro - Complex tasks, detailed responses",
        "gemini-2.5-flash-lite": "üöÄ Flash-Lite - Fastest, high-throughput",
    }
    
    model_name = st.selectbox(
        "Select Gemini Model",
        options=list(model_options.keys()),
        format_func=lambda x: model_options[x],
        help="""
        ‚Ä¢ **Pro**: Complex tasks, large datasets, 1M+ token context
        ‚Ä¢ **Flash**: Real-time apps, balanced quality/speed
        ‚Ä¢ **Flash-Lite**: High-speed, cost-effective classification
        """
    )
    
    st.divider()
    
    # Show history toggle
    show_history = st.checkbox("Show Evaluation History", value=False)

# Main content
col1, col2 = st.columns([1, 1])

with col1:
    st.header("üìù Input")
    
    question = st.text_area(
        "Question",
        height=150,
        placeholder="Enter the question or prompt...",
        help="The original question that was asked"
    )
    
    answer = st.text_area(
        "Model Answer",
        height=200,
        placeholder="Enter the model-generated answer to evaluate...",
        help="The answer generated by your model"
    )
    
    evaluate_button = st.button("üîç Evaluate", type="primary", use_container_width=True)

with col2:
    st.header("üìä Results")
    
    if evaluate_button:
        if not api_key:
            st.error("‚ùå Please enter your Google API Key in the sidebar")
        elif not question or not answer:
            st.error("‚ùå Please provide both a question and an answer")
        else:
            with st.spinner("Evaluating..."):
                # Step 1: LLM-as-a-Judge Evaluation
                feedback, rating = evaluate_with_gemini(question, answer, model_name)
                
                # Step 2: Pydantic Validation
                validation_status = "Valid ‚úÖ"
                completeness_score = 1.0
                
                try:
                    # Try to validate the answer as a structured response
                    # This is a simple validation - you can customize based on your needs
                    model_answer = ModelAnswer(content=answer)
                except ValidationError as e:
                    validation_status = f"Invalid ‚ùå\n{str(e)}"
                    completeness_score = 0.0
                
                # Step 3: Log to CSV
                try:
                    log_evaluation(
                        model=model_name,
                        question=question,
                        answer=answer,
                        judge_feedback=feedback,
                        total_rating=rating,
                        validation_status=validation_status,
                        completeness_score=completeness_score
                    )
                    csv_saved = True
                except Exception as e:
                    csv_saved = False
                    st.error(f"Error saving to CSV: {str(e)}")
                
                # Display results
                st.subheader("ü§ñ LLM Judge Feedback")
                st.markdown(feedback)
                
                st.divider()
                
                col_a, col_b = st.columns(2)
                
                with col_a:
                    st.metric("Total Rating", f"{rating}/4" if rating else "N/A")
                
                with col_b:
                    st.metric("Completeness Score", f"{completeness_score:.1f}")
                
                st.divider()
                
                st.subheader("‚úÖ Pydantic Validation")
                if completeness_score == 1.0:
                    st.success(validation_status)
                else:
                    st.error("Validation Failed")
                    with st.expander("Show validation errors"):
                        st.code(validation_status)
                
                if csv_saved:
                    st.success("‚úÖ Results saved to evaluations.csv")

# Show evaluation history if toggled
if show_history:
    st.divider()
    st.header("üìö Evaluation History")
    
    history_df = get_evaluation_history()
    
    if not history_df.empty:
        # Display summary metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Evaluations", len(history_df))
        
        with col2:
            avg_rating = history_df['total_rating'].dropna().mean()
            st.metric("Average Rating", f"{avg_rating:.2f}" if not pd.isna(avg_rating) else "N/A")
        
        with col3:
            avg_completeness = history_df['completeness_score'].mean()
            st.metric("Avg Completeness", f"{avg_completeness:.2f}")
        
        with col4:
            models_used = history_df['model'].nunique()
            st.metric("Models Used", models_used)
        
        st.divider()
        
        # Display the dataframe
        st.dataframe(
            history_df,
            use_container_width=True,
            hide_index=True
        )
        
        # Download button
        csv_data = history_df.to_csv(index=False)
        st.download_button(
            label="üì• Download CSV",
            data=csv_data,
            file_name="evaluations.csv",
            mime="text/csv"
        )
    else:
        st.info("No evaluation history found. Run some evaluations to see history here.")

# Footer
st.divider()
st.markdown(
    """
    <div style='text-align: center; color: gray; font-size: 0.9em;'>
    üí° Tip: Configure your GOOGLE_API_KEY in a .env file for automatic loading
    </div>
    """,
    unsafe_allow_html=True
)

# Import pandas for history metrics
import pandas as pd

