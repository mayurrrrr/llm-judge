"""
LLM Evaluation Playground - Streamlit App
"""
import streamlit as st
import os
from dotenv import load_dotenv
from pydantic import ValidationError
import pandas as pd

from models import ModelAnswer
from judge import (
    configure_gemini,
    evaluate_with_gemini,
    EVALUATION_PROMPT
)
from logger import log_evaluation, get_evaluation_history


# Load environment variables
load_dotenv()

# Page configuration
st.set_page_config(
    page_title="LLM Evaluation Playground",
    page_icon="üßæ",
    layout="wide"
)

# Title
st.title("üßæ LLM Evaluation Playground")
st.markdown("Evaluate model-generated outputs using LLM-as-a-Judge and Pydantic validation")

with st.expander("View the Main Evaluation Prompt"):
    st.code(EVALUATION_PROMPT, language='markdown')

# Sidebar for configuration
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    # API Key input
    api_key = st.text_input(
        "Google API Key",
        type="password",
        value=os.getenv("GOOGLE_API_KEY", ""),
        help="Enter your Google Gemini API key"
    )
    
    if api_key:
        try:
            configure_gemini(api_key)
            st.success("‚úÖ API Key configured")
        except Exception as e:
            st.error(f"‚ùå Error configuring API: {str(e)}")
    else:
        st.warning("‚ö†Ô∏è Please enter your Google API Key")
    
    st.divider()
    
    # Model selection
    model_options = {
        "gemini-2.5-flash": "‚ö° Flash - Balanced speed and quality (recommended)",
        "gemini-2.5-pro": "üß† Pro - Complex tasks, detailed responses",
        "gemini-2.5-flash-lite": "üöÄ Flash-Lite - Fastest, high-throughput",
    }
    
    model_name = st.selectbox(
        "Select Gemini Model",
        options=list(model_options.keys()),
        format_func=lambda x: model_options[x],
        help="""
        ‚Ä¢ **Pro**: Complex tasks, large datasets, 1M+ token context
        ‚Ä¢ **Flash**: Real-time apps, balanced quality/speed
        ‚Ä¢ **Flash-Lite**: High-speed, cost-effective classification
        """
    )
    
    st.divider()
    
    # Temperature slider
    temperature = st.slider(
        "Set Judge Temperature",
        min_value=0.0,
        max_value=1.0,
        value=0.5,
        step=0.1,
        help="Controls the randomness of the judge's output. Higher is more creative."
    )
    
    # Show history toggle
    show_history = st.checkbox("Show Evaluation History", value=False)

# Main content
individual_tab, batch_tab = st.tabs(["Individual Evaluation", "Batch Evaluation"])

with individual_tab:
    col1, col2 = st.columns([1, 1])

    with col1:
        st.header("üìù Input")
        
        question = st.text_area(
            "Question",
            height=150,
            placeholder="Enter the question or prompt...",
            help="The original question that was asked"
        )
        
        answer = st.text_area(
            "Model Answer",
            height=200,
            placeholder="Enter the model-generated answer to evaluate...",
            help="The answer generated by your model"
        )
        
        evaluate_button = st.button("üîç Evaluate", type="primary", use_container_width=True)

    with col2:
        st.header("üìä Results")
        
        if evaluate_button:
            if not api_key:
                st.error("‚ùå Please enter your Google API Key in the sidebar")
            elif not question or not answer:
                st.error("‚ùå Please provide both a question and an answer")
            else:
                with st.spinner("Evaluating..."):
                    # Step 1: LLM-as-a-Judge Evaluation
                    feedback, scores, prompt = evaluate_with_gemini(
                        question,
                        answer,
                        model_name,
                        temperature
                    )
                    
                    # Step 2: Pydantic Validation
                    validation_status = "Valid ‚úÖ"
                    
                    try:
                        # Try to validate the answer as a structured response
                        # This is a simple validation - you can customize based on your needs
                        model_answer = ModelAnswer(content=answer)
                    except ValidationError as e:
                        validation_status = f"Invalid ‚ùå\n{str(e)}"
                    
                    # Step 3: Log to CSV
                    try:
                        log_evaluation(
                            model=model_name,
                            temperature=temperature,
                            question=question,
                            answer=answer,
                            judge_feedback=feedback,
                            judge_prompt=prompt,
                            total_rating=scores["total"],
                            validation_status=validation_status,
                            relevance_score=scores["relevance"],
                            clarity_score=scores["clarity"],
                            consistency_score=scores["consistency"],
                            creativity_score=scores["creativity"]
                        )
                        csv_saved = True
                    except Exception as e:
                        csv_saved = False
                        st.error(f"Error saving to CSV: {str(e)}")
                    
                    # Display results
                    st.subheader("ü§ñ LLM Judge Feedback")
                    st.markdown(feedback)
                    
                    st.divider()
                    
                    cols = st.columns(5)
                    
                    with cols[0]:
                        st.metric("Total Rating (1-10)", f"{scores['total']}/10" if scores['total'] is not None else "N/A")
                    
                    with cols[1]:
                        st.metric("Relevance", f"{scores['relevance']}/10" if scores['relevance'] is not None else "N/A")
                    
                    with cols[2]:
                        st.metric("Clarity", f"{scores['clarity']}/10" if scores['clarity'] is not None else "N/A")
                    
                    with cols[3]:
                        st.metric("Consistency", f"{scores['consistency']}/10" if scores['consistency'] is not None else "N/A")
                        
                    with cols[4]:
                        st.metric("Creativity", f"{scores['creativity']}/10" if scores['creativity'] is not None else "N/A")

                    st.metric("Validation Status", "Valid" if "Valid" in validation_status else "Invalid")
                    
                    st.divider()
                    
                    with st.expander("Show Judge Prompt"):
                        st.code(prompt, language='markdown')
                    
                    st.subheader("‚úÖ Pydantic Validation")
                    if "Valid" in validation_status:
                        st.success(validation_status)
                    else:
                        st.error("Validation Failed")
                        with st.expander("Show validation errors"):
                            st.code(validation_status)
                    
                    if csv_saved:
                        st.success("‚úÖ Results saved to evaluations.csv")

with batch_tab:
    st.header("üì§ Batch Evaluation from CSV")
    
    uploaded_file = st.file_uploader(
        "Upload a CSV file with 'input' and 'raw_response' columns",
        type="csv"
    )
    
    batch_evaluate_button = st.button("üöÄ Run Batch Evaluation", type="primary", use_container_width=True)
    
    if batch_evaluate_button and uploaded_file:
        if not api_key:
            st.error("‚ùå Please enter your Google API Key in the sidebar")
        else:
            try:
                input_df = pd.read_csv(uploaded_file)
                if "input" not in input_df.columns or "raw_response" not in input_df.columns:
                    st.error("‚ùå CSV must contain 'input' and 'raw_response' columns")
                else:
                    results = []
                    progress_bar = st.progress(0)
                    
                    for index, row in input_df.iterrows():
                        # LLM-as-a-Judge Evaluation
                        feedback, scores, prompt = evaluate_with_gemini(
                            row["input"],
                            row["raw_response"],
                            model_name,
                            temperature
                        )
                        
                        # Pydantic Validation
                        try:
                            ModelAnswer(content=row["raw_response"])
                            validation_status = "Valid"
                        except ValidationError as e:
                            validation_status = f"Invalid: {e}"
                        
                        results.append({
                            "input": row["input"],
                            "raw_response": row["raw_response"],
                            "feedback": feedback,
                            "prompt": prompt,
                            "validation_status": validation_status,
                            **{f"{k}_score": v for k, v in scores.items()}
                        })
                        
                        progress_bar.progress((index + 1) / len(input_df))

                    results_df = pd.DataFrame(results)
                    st.success("‚úÖ Batch evaluation complete!")
                    st.dataframe(results_df)
                    
                    csv_results = results_df.to_csv(index=False)
                    st.download_button(
                        label="üì• Download Results CSV",
                        data=csv_results,
                        file_name="batch_evaluation_results.csv",
                        mime="text/csv"
                    )

            except Exception as e:
                st.error(f"An error occurred: {e}")

# Show evaluation history if toggled
if show_history:
    st.divider()
    st.header("üìö Evaluation History")
    
    history_df = get_evaluation_history()
    
    if not history_df.empty:
        # Display summary metrics
        if 'relevance_score' in history_df.columns and not history_df['relevance_score'].dropna().empty:
            cols = st.columns(5)
        else:
            cols = st.columns(3)
        
        with cols[0]:
            st.metric("Total Evaluations", len(history_df))
        
        with cols[1]:
            st.metric("Avg Rating (1-10)", f"{history_df['total_rating(1-10)'].dropna().mean():.2f}")
            st.metric("Valid Responses", f"{history_df['validation_status'].apply(lambda x: 'Valid' in str(x)).sum()} / {len(history_df)}")
            
        with cols[2]:
            st.metric("Avg Relevance", f"{history_df['relevance_score'].dropna().mean():.2f}")
            st.metric("Avg Clarity", f"{history_df['clarity_score'].dropna().mean():.2f}")

        if len(cols) == 5:
            with cols[3]:
                st.metric("Avg Consistency", f"{history_df['consistency_score'].dropna().mean():.2f}")
                st.metric("Avg Creativity", f"{history_df['creativity_score'].dropna().mean():.2f}")

            with cols[4]:
                 models_used = history_df['model'].nunique()
                 st.metric("Models Used", models_used)
        else:
            with cols[3]:
                models_used = history_df['model'].nunique()
                st.metric("Models Used", models_used)
        
        st.divider()
        
        # Display the dataframe
        st.dataframe(
            history_df,
            use_container_width=True,
            hide_index=True
        )
        
        # Download button
        csv_data = history_df.to_csv(index=False)
        st.download_button(
            label="üì• Download CSV",
            data=csv_data,
            file_name="evaluations.csv",
            mime="text/csv"
        )
    else:
        st.info("No evaluation history found. Run some evaluations to see history here.")

# Footer
st.divider()
st.markdown(
    """
    <div style='text-align: center; color: gray; font-size: 0.9em;'>
    üí° Tip: Configure your GOOGLE_API_KEY in a .env file for automatic loading
    </div>
    """,
    unsafe_allow_html=True
)

# Import pandas for history metrics

